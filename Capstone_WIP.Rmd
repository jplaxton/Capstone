---
title: "Capstone Project"
author: "John Plaxton"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document: default
  pdf_document: default
link-citations: yes
bibliography: citations.bib
biblio-style: apalike
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(readtext)
library(spacyr)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(scales)
#quanteda_options(threads=parallel::detectCores() - 2)
```

```{r quiz 1 output, eval=FALSE, echo=FALSE}
# Week 1 Quiz

# The following code answers the questions for the week 1 quiz.
con <- file("./final/en_US/en_US.twitter.txt", "r") 
size<-file.info("./final/en_US/en_US.blogs.txt")
kb<-size$size/1024
mb<-kb/1024
mb
close(con)

twitter <- readLines(con <- file("./final/en_US/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
length(twitter)


#Blogs file
blogs<-file("./final/en_US/en_US.blogs.txt","r")
blogs_lines<-readLines(blogs)
close(blogs)
summary(nchar(blogs_lines))

# News file
news<-file("./final/en_US/en_US.news.txt","r")
news_lines<-readLines(news)
close(news)
summary(nchar(news_lines))

# Twitter file
twitter<-file("./final/en_US/en_US.twitter.txt","r")
twitter_lines<-readLines(twitter)
close(twitter)
summary(nchar(twitter_lines))

love<-length(grep("love", twitter_lines))
hate<-length(grep("hate", twitter_lines))
love/hate

grep("biostats", twitter_lines, value = T)

grep("A computer once beat me at chess, but it was no match for me at kickboxing", twitter_lines)


#con <- file("./final/en_US/en_US.twitter.txt", "r") 
#readLines(con, 1) ## Read the first line of text 
#readLines(con, 1) ## Read the next line of text 
#readLines(con, 5) ## Read in the next 5 lines of text 
#close(con) ## It's important to close the connection when you are done

#cleanup large charcter vectors
rm(blogs_lines)
rm(news_lines)
rm(twitter_lines)

```

## Create a Corpus

Translating unstructured content from a corpus of information into a meaningful knowledge base is the task of NLP.  It is important to determine what information to keep and how to look for patterns in the structure of that information to distill meaning and context.  Unstructured content is often mixed with structured content, referred to as metadata in the context of NLP.  

Create a Corpus from the provided .txt files...
```{r create_corpus, cache=TRUE}
# The most common problem in loading data into R is misspecifing locations of 
# files or directories. If a path is relative, check where you are using getwd()
# and set the root directory of your project using setwd(). On Windows, you also
# have to replace all \ in a path with /.

textin <- readtext("./final/en_US/*.txt", encoding = "UTF-8", cache = FALSE) # without the encoding="UTF-8", the output (textin) is polluted with Ã¢ characters and skews the frequency analysis to follow
my_corpus <- corpus(textin)
metadoc(my_corpus, "language") <- "english" #adding some structured content (metadata) to the unstructured content
summary(my_corpus, showmeta = TRUE)
```
At this time, no subsetting of the Corpus has been done as the processing time is not unreasonable given the size of the data files.  Further into the project, the Corpus may need to be sampled to bring down the processing time.

## Exploratory Analysis

The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text. The goal of this task is to understand the basic relationships observed in the data and prepare to build initial linguistic models.

A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis.  Tokenization is the process of splitting text into tokens.

The first step in exploratory analysis is to look at the frequency distrbution of single word tokens...
```{r tokenize, cache=TRUE}
dataTokens <- tokens(my_corpus, what='word', remove_numbers = TRUE, 
                     remove_punct = TRUE, remove_symbols = TRUE, 
                     remove_separators = TRUE)
nostop_dataTokens <- tokens_remove(dataTokens, pattern = stopwords('en'))

myDFM <- dfm(dataTokens)
d1 <- textstat_frequency(myDFM, n=20)
p <- ggplot(d1, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "1-Grams - w/Stop Words") +
        theme(plot.title = element_text(hjust=0.5),
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1)) +
        scale_y_continuous(labels = comma)
p
nostop_myDFM <- dfm(nostop_dataTokens)
ns_d1 <- textstat_frequency(nostop_myDFM, n=20)
p <- ggplot(ns_d1, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "1-Grams - Stop Words Removed") +
        theme(plot.title = element_text(hjust=0.5),
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1)) +
        scale_y_continuous(labels = comma)
p
```

Stop words have the highest frequency in the Corpus.  Analysis of the Corpus with stop words removed showed an order of magnitude difference in token frequency.

Through analysis of the UTF-coding and evaluating/filtering tokens based on foreign language dictionaries, an assessment of the quantity of foreign words may be done.  This analysis would be done as a preprocessing step.

The next step in exploratory analysis is to look at the frequency distribution of ngrams, i.e. two and three word combinations.  The ngrams were investigated by looking at frequency of 2 and 3 grams with and without stopwords.

```{r ngram_freq, cache=TRUE}
ngram2 <- tokens_ngrams(dataTokens, n = 2L, skip = 0L, concatenator = " ") 
nostop_ngram2 <- tokens_ngrams(nostop_dataTokens, n=2L, skip = 0L, 
                               concatenator = " ")

d2 <- textstat_frequency(dfm(ngram2),n=20)
p <- ggplot(d2, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "2-Grams - w/Stop Words") +
        theme(plot.title = element_text(hjust=0.5), 
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1)) +
        scale_y_continuous(labels = comma)
p
ns_d2 <- textstat_frequency(dfm(nostop_ngram2),n=20)
p <- ggplot(ns_d2, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "2-Grams - Stop Words Removed") +
        theme(plot.title = element_text(hjust=0.5), 
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1)) +
        scale_y_continuous(labels = comma)
p

ngram3 <- tokens_ngrams(dataTokens, n = 3L, skip = 0L, concatenator = " ")
nostop_ngram3 <- tokens_ngrams(nostop_dataTokens, n = 3L, skip = 0L, 
                               concatenator = " ")

d3 <- textstat_frequency(dfm(ngram3),n=20)
p <- ggplot(d3, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "3-Grams - w/Stop Words") +
        theme(plot.title = element_text(hjust=0.5),
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1)) +
        scale_y_continuous(labels = comma)
p

ns_d3 <- textstat_frequency(dfm(nostop_ngram3),n=20)
p <- ggplot(ns_d3, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "3-Grams - Stop Words Removed") +
        theme(plot.title = element_text(hjust=0.5),
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1)) +
        scale_y_continuous(labels = comma)
p
```

Again, the value of removing stop words is seen as the highest frequency ngrams are dominated by combinations of stop words.

Final step in exploratory anaylsis is to look at how large does a frequecy sorted dictionary needs to be.  Based upon the frequency evaluation of the Corpus used, the following graph demonstrates the coverage as words with decreasing usage frequency are added.  The first graph was built with the stop words included. 

```{r uniquewords, echo=FALSE}
termFreq <- sort(colSums(as.matrix(myDFM)), decreasing=TRUE) 
one_word_freq <- data.frame(word=names(termFreq), freq=termFreq) 
#This function will assist us in getting the number of words needed 
#to get to the required percentage
get_num_words <- function(pct_needed) { 
    i = 1
    pct = 0
    while (pct <= pct_needed){
      pct = pct + (one_word_freq$freq[i]/sum(one_word_freq$freq) * 100)
      i = i + 1
    }
    return(i)
}
#Create new dataframe with first row
df_perc_freq <- data.frame(get_num_words(10))
f <- 20
while (f<100) {
    df_perc_freq <- rbind(df_perc_freq, get_num_words(f))
    f <- f+ 10
}
df_perc_freq <- cbind(df_perc_freq, c(10,20,30,40,50,60,70,80,90))
colnames(df_perc_freq) <- c("numWords", "Pct")
p <- ggplot(df_perc_freq, aes(x=Pct, y = numWords)) +
        geom_line() +
        geom_point() +
        labs(x = "Percent", y = "Number of Words", 
             title = "Number of Unique Words Needed to Cover all Instances") +
        theme(plot.title = element_text(hjust=0.5))
p
```

With stop words removed, the Unique Word Coverage graph looks as follows:

```{r uniquewords_nostop, echo=FALSE}
termFreq <- sort(colSums(as.matrix(nostop_myDFM)), decreasing=TRUE) 
one_word_freq <- data.frame(word=names(termFreq), freq=termFreq) 
#This function will assist us in getting the number of words needed 
#to get to the required percentage
get_num_words <- function(pct_needed) { 
    i = 1
    pct = 0
    while (pct <= pct_needed){
      pct = pct + (one_word_freq$freq[i]/sum(one_word_freq$freq) * 100)
      i = i + 1
    }
    return(i)
}
#Create new dataframe with first row
df_perc_freq <- data.frame(get_num_words(10))
f <- 20
while (f<100) {
    df_perc_freq <- rbind(df_perc_freq, get_num_words(f))
    f <- f+ 10
}
df_perc_freq <- cbind(df_perc_freq, c(10,20,30,40,50,60,70,80,90))
colnames(df_perc_freq) <- c("numWords", "Pct")
p <- ggplot(df_perc_freq, aes(x=Pct, y = numWords)) +
        geom_line() +
        geom_point() +
        labs(x = "Percent", y = "Number of Words", 
             title = "Number of Unique Words Needed to Cover all Instances") +
        theme(plot.title = element_text(hjust=0.5))
p
```

Number of words needed for 50% coverage by a frequency sorted, no stop words dictionary is `r format(get_num_words(50), scientific=FALSE,big.mark=",")`.

Number of words needed for 90% coverage by a frequency sorted, no stop words dictionary is `r format(get_num_words(90),scientific=FALSE,big.mark=",")`. 

Removing stop words increases the size of the frequency sorted dictionary.  Stemming may be applied to increase coverage of lower frequency variants of base words without increasing the size of the dictionary.  The impact of stemming on the ngrams would need more exploration - how are word variant usage captured in the ngrams?

## Concept for Next Word Prediction Model Development

Starting with the 90%-coverage, frequency-sorted, no-stop-word dictionary, a prediction model would need to be built that uses 2-gram and 3-gram frequency data at the predictor of the next word.  It seems that the Shiny app would start with subsetting the 2-gram and 3-gram frequency DFMs by the word entered by the user.  Subsetting should stream line the predictor model as it would only look at the frequency numbers relevant to the starting word.  I am going to need to iterate through these ideas.

## Further Insights & Challenges

Editorial comment on the cited reference of Text Mining Infrastructure in R...it is outdated.  Much of the content does not work as the TM package has been revised numerous times since the papaer was written.  Functions have been renamed, deprecated, or rewritten creating churn for a researacher attempting to duplicate examples.  It is best not to attempt to replicate but just read for intent.

```{r cleanup, include=FALSE}
#quanteda_options(threads=2)
rm(textin)
rm(myCorpus)
rm(dataTokens)
rm(ngram)
rm(myStemMat)
```