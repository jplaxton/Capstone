---
title: "Capstone Project"
author: "John Plaxton"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: ['citations.bib']
biblio-style: "apalike"
link-citations: true
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(readtext)
library(spacyr)
library(tidyverse)
library(tidytext)
library(ggplot2)
#quanteda_options(threads=parallel::detectCores() - 2)
```

```{r quiz 1 output, eval=FALSE, echo=FALSE}
# Week 1 Quiz

# The following code answers the questions for the week 1 quiz.
con <- file("./final/en_US/en_US.twitter.txt", "r") 
size<-file.info("./final/en_US/en_US.blogs.txt")
kb<-size$size/1024
mb<-kb/1024
mb
close(con)

twitter <- readLines(con <- file("./final/en_US/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
length(twitter)


#Blogs file
blogs<-file("./final/en_US/en_US.blogs.txt","r")
blogs_lines<-readLines(blogs)
close(blogs)
summary(nchar(blogs_lines))

# News file
news<-file("./final/en_US/en_US.news.txt","r")
news_lines<-readLines(news)
close(news)
summary(nchar(news_lines))

# Twitter file
twitter<-file("./final/en_US/en_US.twitter.txt","r")
twitter_lines<-readLines(twitter)
close(twitter)
summary(nchar(twitter_lines))

love<-length(grep("love", twitter_lines))
hate<-length(grep("hate", twitter_lines))
love/hate

grep("biostats", twitter_lines, value = T)

grep("A computer once beat me at chess, but it was no match for me at kickboxing", twitter_lines)


#con <- file("./final/en_US/en_US.twitter.txt", "r") 
#readLines(con, 1) ## Read the first line of text 
#readLines(con, 1) ## Read the next line of text 
#readLines(con, 5) ## Read in the next 5 lines of text 
#close(con) ## It's important to close the connection when you are done

#cleanup large charcter vectors
rm(blogs_lines)
rm(news_lines)
rm(twitter_lines)

```

## Create a Corpus

Create a Corpus from the provided .txt files...

Translating unstructured content from a corpus of information into a meaningful knowledge base is the task of NLP.  It is important to determine what information to keep and how to look for patterns in the structure of that information to distill meaning and context.  Unstructured content is often mixed with structured content, referred to as metadata in the context of NLP.  

```{r create_corpus, cache=TRUE}
# The most common problem in loading data into R is misspecifing locations of 
# files or directories. If a path is relative, check where you are using getwd()
# and set the root directory of your project using setwd(). On Windows, you also
# have to replace all \ in a path with /.

textin <- readtext("./final/en_US/*.txt", encoding = "UTF-8", cache = FALSE) # without the encoding="UTF-8", the output (textin) is polluted with Ã¢ characters and skews the frequency analysis to follow
my_corpus <- corpus(textin)
metadoc(my_corpus, "language") <- "english" #adding some structured content (metadata) to the unstructured content
summary(my_corpus, showmeta = TRUE)
```

## Exploratory Analysis

The first step in building a predictive model for text is understanding the distribution and relationship between the words, tokens, and phrases in the text. The goal of this task is to understand the basic relationships you observe in the data and prepare to build your first linguistic models.

A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens.

###Tasks to accomplish

1. **Exploratory analysis** - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.

2. **Understand frequencies of words and word pairs** - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

###Questions to consider

1. Some words are more frequent than others - what are the distributions of word frequencies?

```{r tokenize, cache=TRUE}
dataTokens <- tokens(my_corpus, what='word', remove_numbers = TRUE, 
                     remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)
nostop_dataTokens <- tokens_remove(dataTokens, pattern = stopwords('en'))

myDFM <- dfm(dataTokens)
d1 <- textstat_frequency(myDFM, n=20)
p <- ggplot(d1, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "1-Grams - w/Stop Words") +
        theme(plot.title = element_text(hjust=0.5),
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1))
p
nostop_myDFM <- dfm(nostop_dataTokens)
ns_d1 <- textstat_frequency(nostop_myDFM, n=20)
p <- ggplot(ns_d1, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        geom_text(aes(label = frequency), size=2, vjust = -0.2) +
        labs(x = NULL, y = "Frequency", title = "1-Grams - Stop Words Removed") +
        theme(plot.title = element_text(hjust=0.5),
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1))
p
```

2. What are the frequencies of 2-grams and 3-grams in the dataset?

The ngrams were investigated by looking at frequency of 2 and 3 grams with and without stopwords.
```{r ngram_freq, cache=TRUE}
ngram2 <- tokens_ngrams(dataTokens, n = 2L, skip = 0L, concatenator = " ") 
nostop_ngram2 <- tokens_ngrams(nostop_dataTokens, n=2L, skip = 0L, concatenator = " ")

d2 <- textstat_frequency(dfm(ngram2),n=20)
p <- ggplot(d2, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "2-Grams - w/Stop Words") +
        theme(plot.title = element_text(hjust=0.5), 
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1))
p
ns_d2 <- textstat_frequency(dfm(nostop_ngram2),n=20)
p <- ggplot(ns_d2, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "2-Grams - Stop Words Removed") +
        theme(plot.title = element_text(hjust=0.5), 
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1))
p

ngram3 <- tokens_ngrams(dataTokens, n = 3L, skip = 0L, concatenator = " ")
nostop_ngram3 <- tokens_ngrams(nostop_dataTokens, n = 3L, skip = 0L, concatenator = " ")

d3 <- textstat_frequency(dfm(ngram3),n=20)
p <- ggplot(d3, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "3-Grams - w/Stop Words") +
        theme(plot.title = element_text(hjust=0.5),
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1))
p

ns_d3 <- textstat_frequency(dfm(nostop_ngram3),n=20)
p <- ggplot(ns_d3, aes(x=reorder(feature, -frequency), y = frequency)) +
        geom_bar(stat = "identity") +
        labs(x = NULL, y = "Frequency", title = "3-Grams - Stop Words Removed") +
        theme(plot.title = element_text(hjust=0.5),
              axis.text.x = element_text(angle=30, hjust = 1, vjust = 1))
p
```

3. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?

```{r uniquewords}
termFreq <- sort(colSums(as.matrix(myDFM)), decreasing=TRUE) 
one_word_freq <- data.frame(word=names(termFreq), freq=termFreq) 
#This function will assist us in getting the number of words needed 
#to get to the required percentage
get_num_words <- function(pct_needed) { 
    i = 1
    pct = 0
    while (pct <= pct_needed){
      pct = pct + (one_word_freq$freq[i]/sum(one_word_freq$freq) * 100)
      i = i + 1
    }
    return(i)
}

#Create new dataframe with first row
df_perc_freq <- data.frame(get_num_words(10))
f <- 20
  
while (f<100) {
    df_perc_freq <- rbind(df_perc_freq, get_num_words(f))
    f <- f+ 10
}
  
df_perc_freq <- cbind(df_perc_freq, c(10,20,30,40,50,60,70,80,90))
colnames(df_perc_freq) <- c("numWords", "Pct")

p <- ggplot(df_perc_freq, aes(x=Pct, y = numWords)) +
        geom_line() +
        geom_point() +
        labs(x = "Percent", y = "Number of Words", 
             title = "Number of Unique Words Needed to Cover all Instances") +
        theme(plot.title = element_text(hjust=0.5))
p
```

4. How do you evaluate how many of the words come from foreign languages?

Through analysis of the UTF-coding and evaluating filtering results based on foreign language dictionaries, an assessment of the quantity of foreign words may be done.

5. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

Stemming may be applied to increase coverage of lower frequency varients of base words.  A concern may be the impact on the ngrams - is the varient usage contextual when creating the ngrams?

## Further Insights & Challenges

Editorial comment on the cited reference of Text Mining Infrastructure in R...it is outdated.  Much of the content does not work as the TM package has been revised numerous times since the papaer was written.  Functions have been renamed, deprecated, or rewritten creating churn for a researacher attempting to duplicate examples.  It is best not to attempt to replicate but just read for intent.

```{r cleanup, include=FALSE}
#quanteda_options(threads=2)
rm(textin)
rm(myCorpus)
rm(dataTokens)
rm(ngram)
rm(myStemMat)
```